

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Focus on EM sampler &mdash; qolmat 0.0.9 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="_static/doctools.js"></script>
        <script async="async" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Imputation examples" href="examples/imputation_example.html" />
    <link rel="prev" title="Focus on RPCA" href="RPCA.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> qolmat
          

          
          </a>

          
            
            
              <div class="version">
                0.0.9
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Brief explanation</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="explanation.html">Qolmat methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="RPCA.html">Focus on RPCA</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Focus on EM sampler</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#basics-of-gaussians">Basics of Gaussians</a></li>
<li class="toctree-l2"><a class="reference internal" href="#em-algorithm">EM algorithm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#how-it-works">How it works</a></li>
<li class="toctree-l3"><a class="reference internal" href="#computation">Computation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#multivariate-time-series">Multivariate time series</a></li>
<li class="toctree-l2"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Simple examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="examples/imputation_example.html">Imputation examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples/RPCA_example.html">RPCA examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api.html">Qolmat API</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">qolmat</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Focus on EM sampler</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/EM_sampler.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="focus-on-em-sampler">
<h1>Focus on EM sampler<a class="headerlink" href="#focus-on-em-sampler" title="Permalink to this heading">¶</a></h1>
<p>This method allows the imputation of missing values in multivariate data using a multivariate Gaussian model
via EM algorithm.</p>
<section id="basics-of-gaussians">
<h2>Basics of Gaussians<a class="headerlink" href="#basics-of-gaussians" title="Permalink to this heading">¶</a></h2>
<p>We assume the data <span class="math notranslate nohighlight">\(\mathbf{X} \in \mathbb{R}^{n \times p}\)</span> follows a
multivariate Gaussian distribution <span class="math notranslate nohighlight">\(\mathcal{N}(\mathbf{m}, \mathbf{\Sigma})\)</span>.
Hence, the density of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[p(\mathbf{x}) = \frac{1}{\sqrt{\det (2 \pi \mathbf{\Sigma})}} \exp \left[-\frac{1}{2} (\mathbf{x} - \mathbf{m})^\top \mathbf{\Sigma}^{-1}  (\mathbf{x} - \mathbf{m}) \right]\]</div>
<p>We define <span class="math notranslate nohighlight">\(\Omega := \{ (i,j) \, | \, X_{ij} \text{ is observed} \}\)</span>,
and <span class="math notranslate nohighlight">\(\Omega_i := \{ j \, | \, X_{ij} \text{ is observed} \}\)</span>.
The complementary of these sets are <span class="math notranslate nohighlight">\(\Omega^c := \{ (i,j) \, | \, X_{ij} \text{ is missing} \}\)</span>
and <span class="math notranslate nohighlight">\(\Omega_i^c := \{ j \, | \, X_{ij} \text{ is missing} \}\)</span>.</p>
<p>Each row <span class="math notranslate nohighlight">\(i\)</span> of the matrix represents a time, <span class="math notranslate nohighlight">\(1 \leq  i \leq n\)</span>,
and each column <span class="math notranslate nohighlight">\(j\)</span> represents a variable, <span class="math notranslate nohighlight">\(1 \leq  j \leq m\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{x}_i \in \mathbb{R}^p\)</span> be an observation, i.e. <span class="math notranslate nohighlight">\(\mathbf{x}_i \overset{iid}{\sim} \mathcal{N}_{\mathbf{x}_i}(\mu, \mathbf{\Sigma})\)</span>.
We can rearrange the entries of <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span> such that we can write</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{x}_i =
    \begin{bmatrix}
        \mathbf{x}_{i, \Omega} \\
        \mathbf{x}_{i, \Omega^c}
    \end{bmatrix}
    \sim
    \mathcal{N}_{\mathbf{x}_i}
    \left(
        \begin{bmatrix}
            \mathbf{\mu}_{\Omega_i} \\
            \mathbf{\mu}_{\Omega^c_i}
        \end{bmatrix},
        \begin{bmatrix}
            \mathbf{\Sigma}_{\Omega_i \Omega_i} &amp; \mathbf{\Sigma}_{\Omega_i \Omega^c_i} \\
            \mathbf{\Sigma}_{\Omega^c_i \Omega_i} &amp; \mathbf{\Sigma}_{\Omega^c_i \Omega^c_i}
        \end{bmatrix}
    \right)\end{split}\]</div>
<p>Thus formulated, the conditional distributions can be expressed as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{l}
    p(\mathbf{x}_{i, \Omega^c_i} | \mathbf{x}_{i, \Omega})
        = \mathcal{N}_{\mathbf{x}_i}(\tilde{\mu_i}, \tilde{\mathbf{\Sigma}_{i,\Omega_i^c}}) \\
    \text{where } \tilde{\mu}_i =
        \mu_{\Omega^c_i} + \mathbf{\Sigma}_{\Omega^c_i \Omega_i} \mathbf{\Sigma}^{-1}_{\Omega_i \Omega_i} (\mathbf{x}_{i, \Omega_i} - \mathbf{\mu}_{\Omega_i}) \\
    \phantom{\text{ where }} \tilde{\mathbf{\Sigma}}_{i,\Omega_i^c} =
        \mathbf{\Sigma}_{\Omega^c_i \Omega^c_i} - \mathbf{\Sigma}_{\Omega^c_i \Omega_i} \mathbf{\Sigma}^{-1}_{\Omega_i \Omega_i} \mathbf{\Sigma}_{\Omega_i \Omega^c_i}
\end{array}\end{split}\]</div>
<p>Note, that the covariance matrices are the Schur complement of the block matrix.</p>
<p>Recall also the mean of square forms, i.e.</p>
<div class="math notranslate nohighlight">
\[E \left[ (\mathbf{x} - \mathbf{m}')^\top \mathbf{A} (\mathbf{x} - \mathbf{m}') \right] = (\mathbf{m} - \mathbf{m}')^\top \mathbf{A} (\mathbf{m} - \mathbf{m}') + \text{Tr}(\mathbf{A}\mathbf{\Sigma}),\]</div>
<p>for all square matrices <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>.</p>
</section>
<section id="em-algorithm">
<h2>EM algorithm<a class="headerlink" href="#em-algorithm" title="Permalink to this heading">¶</a></h2>
<p>The EM algorithm is an optimisation algorithm that maximises the “expected complete data log likelihood” by some iterative
means under the (conditional) distribution of unobserved components.
In this way it is possible to calculate the statistics of interest.</p>
<section id="how-it-works">
<h3>How it works<a class="headerlink" href="#how-it-works" title="Permalink to this heading">¶</a></h3>
<p>We start with a first estimation <span class="math notranslate nohighlight">\(\mathbf{\hat{X}}\)</span> of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>, obtained via a simple
imputation method, i.e. linear interpolation.</p>
<p>the expectation step (or E-step) at iteration <em>t</em> computes:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ll}
    \mathcal{Q}(\theta \, | \, \theta^{(t)}) &amp;:= &amp;E \left[ \log L(\theta ; \mathbf{X}) \, | \, \mathbf{X}_{\Omega}, \theta^{(t)} \right] \\
    &amp; = &amp; \sum_{i=1}^n E \left[ \log L(\theta ; \mathbf{x}_i) \, | \, \mathbf{x}_{i, \Omega_i}, \theta^{(t)} \right].
\end{array}\end{split}\]</div>
<p>The maximization step (or M-step) at iteration <em>t</em> finds:</p>
<div class="math notranslate nohighlight">
\[\theta^{(t+1)} := \underset{\theta}{\mathrm{argmax}} \left\{ \mathcal{Q} \left( \theta \, | \, \theta^{(t)} \right) \right\}.\]</div>
<p>These two steps are repeated until the parameter estimate converges.</p>
</section>
<section id="computation">
<h3>Computation<a class="headerlink" href="#computation" title="Permalink to this heading">¶</a></h3>
<p>At iteration <span class="math notranslate nohighlight">\(\textit{t}\)</span> with <span class="math notranslate nohighlight">\(\theta^{(t)} = (\mu^{(t)}, \mathbf{\Sigma}^{(t)})\)</span>, let’s
<span class="math notranslate nohighlight">\(\mathbf{x}_i \sim \mathcal{N}_p(\mu, \Sigma)\)</span>. The expected log likelihhod is equal to</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ll}
    \mathcal{Q}_i(\theta \, | \, \theta^{(t)})
    &amp;=&amp; E \left[ - \frac{1}{2} \log \det \mathbf{\Sigma} - \frac{1}{2} (\mathbf{x}_i - \mu)^\top \mathbf{\Sigma}^{-1} (\mathbf{x}_i - \mu) \, | \, \mathbf{x}_{i, \Omega_i}, \theta^{(t)} \right] \\
    &amp;=&amp; - \frac{1}{2} \log \det \mathbf{\Sigma} - \frac{1}{2} \Big(
            (\mathbf{x}_{i,\Omega_i} - \mu_{\Omega_i})^\top \mathbf{\Sigma}_{\Omega_i\Omega_i}^{-1} (\mathbf{x}_{i,\Omega_i} - \mu_{\Omega_i})
            \\
            &amp;&amp; \phantom{- \frac{1}{2}}  +
            2 (\mathbf{x}_{i,\Omega_i} - \mu_{\Omega_i})^\top \mathbf{\Sigma}_{\Omega_i\Omega^c_i}^{-1} E \left[ \mathbf{x}_{i,\Omega^c_i} - \mu_{\Omega^c_i} \, | \, \mathbf{x}_{i, \Omega_i}, \theta^{(t)} \right]
            \\
            &amp;&amp; \phantom{- \frac{1}{2}}  +
            E \left[ (\mathbf{x}_{i,\Omega^c_i} - \mu_{\Omega^c_i})^\top \mathbf{\Sigma}_{\Omega^c_i\Omega^c_i}^{-1} (\mathbf{x}_{i,\Omega^c_i} - \mu_{\Omega^c_i}) \, | \, \mathbf{x}_{i, \Omega_i}, \theta^{(t)} \right]
            \Big) \\
    &amp;=&amp; - \frac{1}{2} \log \det \mathbf{\Sigma}
        - \frac{1}{2} \Big(
            (\mathbf{x}_{i,\Omega_i} - \mu_{\Omega_i})^\top \mathbf{\Sigma}_{\Omega_i\Omega_i}^{-1} (\mathbf{x}_{i,\Omega_i} - \mu_{\Omega_i})
            \\
            &amp;&amp; \phantom{- \frac{1}{2}}  +
            2 (\mathbf{x}_{i,\Omega_i} - \mu_{\Omega_i})^\top \mathbf{\Sigma}_{\Omega_i\Omega^c_i}^{-1} (\tilde{\mu}_{i}^{(t)} - \mu_{\Omega^c_i})
            \\
            &amp;&amp; \phantom{- \frac{1}{2}}  +
            (\tilde{\mu}_{i}^{(t)} - \mu_{\Omega^c_i})^\top \mathbf{\Sigma}^{-1}_{\Omega_i^c\Omega_i^c} (\tilde{\mu}_{i}^{(t)} - \mu_{\Omega^c_i})
            \\
            &amp;&amp; \phantom{- \frac{1}{2}}  +
            \text{Tr} \left( \mathbf{\Sigma}^{-1}_{\Omega_i^c\Omega_i^c} \tilde{\mathbf{\Sigma}}_{i,\Omega_i^c}^{(t)} \right)
        \Big) \\
    &amp;=&amp; - \frac{1}{2} \log \det \mathbf{\Sigma}
        - \frac{1}{2} \left[
            (\hat{\mathbf{x}}_{i}^{(t)} - \mu)^\top \mathbf{\Sigma}^{-1} (\hat{\mathbf{x}}_{i}^{(t)} - \mu)
            + \text{Tr} \left( \mathbf{\Sigma}^{-1}_{\Omega_i^c\Omega_i^c} \tilde{\mathbf{\Sigma}}_{i,\Omega_i^c}^{(t)} \right)
        \right]
\end{array}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{\mathbf{x}}_{i}^{(t)} = [\hat{x}_{i1}^{(t)}, ..., \hat{x}_{ip}^{(t)}]\)</span>
is the data point such that <span class="math notranslate nohighlight">\(\mathbf{x}_{i\Omega_i^c}^{(t)}\)</span> is replaced by <span class="math notranslate nohighlight">\(\tilde{\mu}_{i}^{(t)}\)</span>.</p>
<p>And finally, one has</p>
<div class="math notranslate nohighlight">
\[\mathcal{Q}(\theta \, | \, \theta^{(t)})  = \sum_{i=1}^n \mathcal{Q}_i(\theta \, | \, \theta^{(t)})\]</div>
<p>For the M-step, one has to find <span class="math notranslate nohighlight">\(\theta\)</span> that maximises the previous expression. Since it is concave, it suffices
to zeroing the derivatives.
For the mean, one has</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{l}
    \nabla_{\mu} \mathcal{Q}(\theta \, | \, \theta^{(t)})
    &amp;= -\frac{1}{2} \sum_{i=1}^n \nabla_{\mu} (\hat{\mathbf{x}}_{i}^{(t)} - \mu)^\top \mathbf{\Sigma}^{-1} (\hat{\mathbf{x}}_{i}^{(t)} - \mu) \\
    &amp;= \mathbf{\Sigma}^{-1} \sum_{i=1}^n  (\hat{\mathbf{x}}_{i}^{(t)} - \mu) \\
    &amp;= 0.
\end{array}\end{split}\]</div>
<p>Therefore</p>
<div class="math notranslate nohighlight">
\[\mu^{(t+1)} = \frac{1}{n} \sum_{i=1}^n \hat{\mathbf{x}}_{i}^{(t)}.\]</div>
<p>For the variance, one has</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ll}
    \nabla_{\Sigma^{-1}} \mathcal{Q}(\theta \, | \, \theta^{(t)})
    &amp;=&amp; \frac{1}{2} \sum_{i=1}^n \nabla_{\Sigma^{-1}} \log \det \Sigma^{-1}
        \\
        &amp;&amp; \phantom{\frac{1}{2}}
        - \nabla_{\Sigma^{-1}} \text{Tr} \left( \mathbf{\Sigma}^{-1}_{\Omega_i^c\Omega_i^c} \tilde{\mathbf{\Sigma}}_i^{(t)} \right)
        \\
        &amp;&amp; \phantom{\frac{1}{2}}
        - \frac{1}{2} \sum_{i=1}^n \nabla_{\Sigma^{-1}} (\hat{\mathbf{x}}_{i}^{(t)} - \mu)^\top \mathbf{\Sigma}^{-1} (\hat{\mathbf{x}}_{i}^{(t)} - \mu) \\
    &amp;=&amp; \frac{1}{2} \left[n \mathbf{\Sigma} - \sum_{i=1}^n \tilde{\mathbf{\Sigma}}_i^{(t)} \right]
        - \frac{1}{2} \sum_{i=1}^n (\hat{\mathbf{x}}_{i}^{(t)} - \mu) (\hat{\mathbf{x}}_{i}^{(t)} - \mu)^\top \\
    &amp;=&amp; 0
\end{array}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\tilde{\mathbf{\Sigma}}_i^{(t)}\)</span> is the <span class="math notranslate nohighlight">\(p \times p\)</span> matrix having zero everywhere
expect the <span class="math notranslate nohighlight">\(\Omega_i^c\Omega_i^c\)</span> block replaced by <span class="math notranslate nohighlight">\(\tilde{\mathbf{\Sigma}}_{i,\Omega_i^c}^{(t)}\)</span>.</p>
<p>Therefore</p>
<div class="math notranslate nohighlight">
\[\mathbf{\Sigma}^{(t+1)} = \frac{1}{n} \sum_{i=1}^n \left[ (\hat{\mathbf{x}}_{i}^{(t)} - \mu) (\hat{\mathbf{x}}_{i}^{(t)} - \mu)^\top + \tilde{\mathbf{\Sigma}}_i^{(t)} \right].\]</div>
<p>We can test the convergence of the algorithm by checking some sort of metric between
two consecutive estimates of the means or the covariances
(it is assumed to converge since the sequences are Cauchy).</p>
<p>Thus, at each iteration, the missing values are replaced either by their corresponding mean or by smapling from
a multivarite normal distribution with fitted mean and variance.
The resulting imputed data is the final imputed array, obtained at convergence.</p>
</section>
</section>
<section id="multivariate-time-series">
<h2>Multivariate time series<a class="headerlink" href="#multivariate-time-series" title="Permalink to this heading">¶</a></h2>
<p>To explicitely take into account the temporal aspect of the data
(temporal correlations), we construct an extended matrix <span class="math notranslate nohighlight">\(\mathbf{X}^{ext}\)</span>
by considering the shifted columns, i.e.
<span class="math notranslate nohighlight">\(\mathbf{X}^{ext} := [\mathbf{X}, \mathbf{X}^{s+1}, \mathbf{X}^{s-1}]\)</span> where
<span class="math notranslate nohighlight">\(\mathbf{X}^{s+1}\)</span> (resp. <span class="math notranslate nohighlight">\(\mathbf{X}^{s-1}\)</span>) is the <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> matrix
where all columns are shifted +1 for one step backward in time (resp. -1 for one step forward in time).
The covariance matrix <span class="math notranslate nohighlight">\(\mathbf{\Sigma}^{ext}\)</span> is therefore richer in information since the presence of additional
(temporal) correlations.</p>
<img alt="_images/extended_matrix.png" src="_images/extended_matrix.png" />
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this heading">¶</a></h2>
<p>[1] Borman, Sean. “The expectation maximization algorithm-a short tutorial.” Submitted for publication 41 (2004).
(<a class="reference external" href="https://www.lri.fr/~sebag/COURS/EM_algorithm.pdf">pdf</a>)</p>
<p>[2] <a class="reference external" href="https://joon3216.github.io/research_materials.html">https://joon3216.github.io/research_materials.html</a></p>
</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="examples/imputation_example.html" class="btn btn-neutral float-right" title="Imputation examples" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="RPCA.html" class="btn btn-neutral float-left" title="Focus on RPCA" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022, Quantmetry

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>